% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/linear_model.R
\docType{data}
\name{Ridge}
\alias{Ridge}
\alias{rsk_Ridge}
\alias{Ridge}
\title{Ridge}
\format{An object of class \code{R6ClassGenerator} of length 24.}
\usage{
rsk_Ridge

Ridge(x, y, alpha = 1, fit_intercept = TRUE, normalize = FALSE,
  copy_X = TRUE, max_iter = NULL, tol = 0.001, solver = "auto",
  random_state = NULL)
}
\arguments{
\item{x}{matrix. Training Data}

\item{y}{matrix. Target Values}

\item{alpha}{{float, array-like}, shape (n_targets)
Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to \code{C^-1} in other linear models such as
LogisticRegression or LinearSVC. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.}

\item{fit_intercept}{boolean
Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).}

\item{normalize}{boolean, optional, default False
If True, the regressors X will be normalized before regression.
This parameter is ignored when \code{fit_intercept} is set to False.
When the regressors are normalized, note that this makes the
hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data.
However, if you wish to standardize, please use
\code{preprocessing.StandardScaler} before calling \code{fit} on an estimator
with \code{normalize=False}.}

\item{copy_X}{boolean, optional, default True
If True, X will be copied; else, it may be overwritten.}

\item{max_iter}{int, optional
Maximum number of iterations for conjugate gradient solver.
For 'sparse_cg' and 'lsqr' solvers, the default value is determined
by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.}

\item{tol}{float
Precision of the solution.}

\item{solver}{{'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}
Solver to use in the computational routines:
\itemize{
\item 'auto' chooses the solver automatically based on the type of data.
\item 'svd' uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than
'cholesky'.
\item 'cholesky' uses the standard scipy.linalg.solve function to
obtain a closed-form solution.
\item 'sparse_cg' uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than 'cholesky' for large-scale data
(possibility to set \code{tol} and \code{max_iter}).
\item 'lsqr' uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest but may not be available
in old scipy versions. It also uses an iterative procedure.
\item 'sag' uses a Stochastic Average Gradient descent. It also uses an
iterative procedure, and is often faster than other solvers when
both n_samples and n_features are large. Note that 'sag' fast
convergence is only guaranteed on features with approximately the
same scale. You can preprocess the data with a scaler from
sklearn.preprocessing.
All last four solvers support both dense and sparse data. However,
only 'sag' supports sparse input when \code{fit_intercept} is True.
}}

\item{random_state}{int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when
shuffling the data. Used only in 'sag' solver.}
}
\description{
Linear least squares with l2 regularization.
This model solves a regression model where the loss function is the linear
least squares function and regularization is given by the l2-norm. Also known
as Ridge Regression or Tikhonov regularization. This estimator has built-in
support for multi-variate regression (i.e., when y is a 2d-array of shape
(n_samples, n_targets).
Read more in the \href{http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression}{User Guide}.
}
\keyword{datasets}
